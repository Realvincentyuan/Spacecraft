<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Support Conversational History in RAG Pipelines with Llama 3</title>
    <link rel="stylesheet" href="../assets/built/screen.css%3Fv=39200ba622.css">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.12.3/tocbot.css">

    <style>
        .gh-content {
            position: relative;
            z-index: 1;
        }

        /*-- Remove underline in the TOC text  */
        .gh-content a {
            text-decoration: none;
        }

        .gh-content img{
            position: relative;
            z-index: 2;
        }

        .gh-toc > .toc-list {
            position: relative;
            
        }

        .toc-list {
            overflow: hidden;
            list-style: none;
            left: 20%;

        }

        @media (min-width: 1300px) {
            .gh-sidebar {
                position: absolute; 
                top: 0;
                bottom: 0;
                margin-top: 4vmin;
                /* grid-column: wide-start / main-start;  Place the TOC to the left of the content */
                grid-column: main-end / wide-end ; /* Place the TOC to the right of the content */

            }
        
            .gh-toc {
                position: sticky; /* On larger screens, TOC will stay in the same spot on the page */
                top: 4vmin;

                /*-- Send the TOC to the back, in case it is overlapped with images  */
                z-index: 1; 

            }
        }

        /*-- Hide TOC on mobile devices */
        @media (max-width: 768px) {
            .gh-toc {
                display: none;
            }
        }

        .gh-toc .is-active-link::before {
            background-color: var(--ghost-accent-color); /* Defines TOC   accent color based on Accent color set in Ghost Admin */

        } 
    </style>

    <meta name="description" content="Add memory to Llama 3 with RAG.">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Spacecraft">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Support Conversational History in RAG Pipelines with Llama 3">
    <meta property="og:description" content="Add memory to Llama 3 with RAG.">
    <meta property="og:url" content="https://realvincentyuan.github.io/Spacecraft/support-conversational-history-in-rag-pipelines-with-llama-3/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta property="article:published_time" content="2024-07-07T20:47:28.000Z">
    <meta property="article:modified_time" content="2024-07-07T21:12:17.000Z">
    <meta property="article:tag" content="Tech">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="GenAI">
    
    <meta property="article:publisher" content="https://www.facebook.com/ghost">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Support Conversational History in RAG Pipelines with Llama 3">
    <meta name="twitter:description" content="Add memory to Llama 3 with RAG.">
    <meta name="twitter:url" content="https://realvincentyuan.github.io/Spacecraft/support-conversational-history-in-rag-pipelines-with-llama-3/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Vincent Yuan">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Tech, AI, GenAI">
    <meta name="twitter:site" content="@ghost">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="800">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Spacecraft",
        "url": "https://realvincentyuan.github.io/Spacecraft/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://realvincentyuan.github.io/Spacecraft/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Vincent Yuan",
        "image": {
            "@type": "ImageObject",
            "url": "https://realvincentyuan.github.io/Spacecraft/content/images/2024/07/avatar.png",
            "width": 752,
            "height": 752
        },
        "url": "https://realvincentyuan.github.io/Spacecraft/author/vincent/",
        "sameAs": []
    },
    "headline": "Support Conversational History in RAG Pipelines with Llama 3",
    "url": "https://realvincentyuan.github.io/Spacecraft/support-conversational-history-in-rag-pipelines-with-llama-3/",
    "datePublished": "2024-07-07T20:47:28.000Z",
    "dateModified": "2024-07-07T21:12:17.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&ixlib=rb-4.0.3&q=80&w=2000",
        "width": 1200,
        "height": 800
    },
    "keywords": "Tech, AI, GenAI",
    "description": "Add memory to Llama 3 with RAG.",
    "mainEntityOfPage": "https://realvincentyuan.github.io/Spacecraft/support-conversational-history-in-rag-pipelines-with-llama-3/"
}
    </script>

    <meta name="generator" content="Ghost 5.87">
    <link rel="alternate" type="application/rss+xml" title="Spacecraft" href="../rss/index.html">
    
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/sodo-search.min.js" data-key="59658b4dc0364cacbc3332171d" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/main.css" data-sodo-search="https://realvincentyuan.github.io/Spacecraft/" crossorigin="anonymous"></script>
    
    <link href="https://realvincentyuan.github.io/Spacecraft/webmentions/receive/" rel="webmention">
    <script defer src="../public/cards.min.js%3Fv=39200ba622"></script><style>:root {--ghost-accent-color: #0f69c2;}</style>
    <link rel="stylesheet" type="text/css" href="../public/cards.min.css%3Fv=39200ba622.css">
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-633EG809GP"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-633EG809GP');
</script>

<!-- Reading progress bar -->
<style>
.reading-progress {
  position: fixed;
  top: 0;
  z-index: 999;
  width: 100%;
  height: 5px; /* Progress bar height */
  background: #c5d2d9; /* Progress bar background color */
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; /* Hide default progress bar */
}

.reading-progress::-webkit-progress-bar {
  background-color: transparent;
}

.reading-progress::-webkit-progress-value {
  background: var(--ghost-accent-color); /* Progress bar color */
}
</style>


<!--   Scroll Top Button  -->
<style>
    /*Scroll to Top CSS*/
    .scroll-top {
		position: fixed;
 		z-index: 50;
		padding: 0;
		right: 30px;
		bottom: 100px;
		opacity: 0;
		visibility: hidden;
		transform: translateY(15px);    
		height: 46px;
		width: 46px;
		cursor: pointer;
		display: flex;
  		align-items: center;
  		justify-content: center;
		border-radius: 50%;
  	transition: all .4s ease;
  	border: none;
  	box-shadow: inset 0 0 0 2px #ccc;
  	color: #ccc;
  	background-color: #fff;
	}

	.scroll-top.is-active {
		opacity: 1;
  		visibility: visible;
  		transform: translateY(0);
	}

	.scroll-top .icon-tabler-arrow-up {
  		position: absolute;
  		stroke-width: 2px;
  		stroke: #333;
	}

	.scroll-top svg path { 
		fill: none; 
	}

	.scroll-top svg.progress-circle path {
		stroke: #333;
		stroke-width: 4;
  		transition: all .4s ease;
	}

	.scroll-top:hover {
  		color: var(--ghost-accent-color);
	}

	.scroll-top:hover .progress-circle path, .scroll-top:hover .icon-tabler-arrow-up {
  		stroke: var(--ghost-accent-color);
	}
          
</style>

    <button class="btn-toggle-round scroll-top js-scroll-top" type="button" title="Scroll to top">
      <svg class="progress-circle" width="100%" height="100%" viewBox="-1 -1 102 102">
        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
      </svg>
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-arrow-up" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="cuurentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <line x1="12" y1="5" x2="12" y2="19" />
        <line x1="18" y1="11" x2="12" y2="5" />
        <line x1="6" y1="11" x2="12" y2="5" />
      </svg>
    </button>



<!-- Code highlights -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>

<body class="post-template tag-tech tag-ai tag-genai  is-head-middle-logo has-serif-title has-serif-body">
<div class="site">

    <header id="gh-head" class="gh-head gh-outer">
        <div class="gh-head-inner gh-inner">
            <div class="gh-head-brand">
                <div class="gh-head-brand-wrapper">
                    <a class="gh-head-logo" href="../index.html">
                            Spacecraft
                    </a>
                </div>
                <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M17.5 17.5L12.5 12.5L17.5 17.5ZM14.1667 8.33333C14.1667 9.09938 14.0158 9.85792 13.7226 10.5657C13.4295 11.2734 12.9998 11.9164 12.4581 12.4581C11.9164 12.9998 11.2734 13.4295 10.5657 13.7226C9.85792 14.0158 9.09938 14.1667 8.33333 14.1667C7.56729 14.1667 6.80875 14.0158 6.10101 13.7226C5.39328 13.4295 4.75022 12.9998 4.20854 12.4581C3.66687 11.9164 3.23719 11.2734 2.94404 10.5657C2.65088 9.85792 2.5 9.09938 2.5 8.33333C2.5 6.78624 3.11458 5.30251 4.20854 4.20854C5.30251 3.11458 6.78624 2.5 8.33333 2.5C9.88043 2.5 11.3642 3.11458 12.4581 4.20854C13.5521 5.30251 14.1667 6.78624 14.1667 8.33333Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</button>
                <button class="gh-burger"></button>
            </div>

            <nav class="gh-head-menu">
                <ul class="nav">
    <li class="nav-home"><a href="../index.html">Home</a></li>
    <li class="nav-tech"><a href="../tag/tech/index.html">Tech</a></li>
    <li class="nav-profession"><a href="../tag/pro/index.html">Profession</a></li>
    <li class="nav-life"><a href="../tag/life/index.html">Life</a></li>
    <li class="nav-about"><a href="../about/index.html">About</a></li>
</ul>

            </nav>

            <div class="gh-head-actions">
                        <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M17.5 17.5L12.5 12.5L17.5 17.5ZM14.1667 8.33333C14.1667 9.09938 14.0158 9.85792 13.7226 10.5657C13.4295 11.2734 12.9998 11.9164 12.4581 12.4581C11.9164 12.9998 11.2734 13.4295 10.5657 13.7226C9.85792 14.0158 9.09938 14.1667 8.33333 14.1667C7.56729 14.1667 6.80875 14.0158 6.10101 13.7226C5.39328 13.4295 4.75022 12.9998 4.20854 12.4581C3.66687 11.9164 3.23719 11.2734 2.94404 10.5657C2.65088 9.85792 2.5 9.09938 2.5 8.33333C2.5 6.78624 3.11458 5.30251 4.20854 4.20854C5.30251 3.11458 6.78624 2.5 8.33333 2.5C9.88043 2.5 11.3642 3.11458 12.4581 4.20854C13.5521 5.30251 14.1667 6.78624 14.1667 8.33333Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</button>
            </div>
        </div>
    </header>


    <div class="site-content">
        
<main class="site-main">


    <article class="post tag-tech tag-ai tag-genai featured">

        <header class="gh-article-header gh-canvas">

        <div class="article-tag post-card-tags">
                <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
        </div>

            <h1 class="gh-article-title">Support Conversational History in RAG Pipelines with Llama 3</h1>

            <div class="post-meta">
                <a href="../author/vincent/index.html">Vincent Yuan</a>&nbsp;·&nbsp;updated on&nbsp;
                <time datetime="2024-07-07">Jul 7, 2024</time>&nbsp;·&nbsp;<span class="gh-card-length">4 min read</span>
            </div>

            <div class="post-meta">
                    <a class="post-tag tag-tech" href="../tag/tech/index.html" title="Tech">Tech</a>
                    <a class="post-tag tag-ai" href="../tag/ai/index.html" title="AI">AI</a>
                    <a class="post-tag tag-genai" href="../tag/genai/index.html" title="GenAI">GenAI</a>
            </div>

                <figure class="gh-article-image">
        <img
            class="post-image"
            srcset="https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;400 400w,
                    https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;750 750w,
                    https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;960 960w,
                    https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1140 1140w,
                    https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1920 1920w"
            sizes="(min-width: 1200px) 960px, 92vw"
            src="https://images.unsplash.com/photo-1715398940416-43e3ec908fa0?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;960"
            alt="Support Conversational History in RAG Pipelines with Llama 3"
        >
            <figcaption><span style="white-space: pre-wrap;">Photo by </span><a href="https://unsplash.com/@vincentyuan87?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span style="white-space: pre-wrap;">Vincent Yuan @USA</span></a><span style="white-space: pre-wrap;"> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span style="white-space: pre-wrap;">Unsplash</span></a></figcaption>
    </figure>
        </header>

        <div class="gh-content gh-canvas">

            <aside class="gh-sidebar"><div class="gh-toc"></div></aside> 

            <p>In Retrieval-Augmented Generation (RAG) pipelines, it's crucial to help chatbots recall previous conversations, as users may ask follow-up questions that rely on earlier context. However, users' prompts might lack sufficient context, assuming previous discussions are still relevant. To tackle this challenge, incorporating chat history into LLMs' question-answering context enables them to retrieve relevant information for new queries.</p><p>This post presents a solution leveraging LangChain, Llama 3-8B, and Ollama, which can efficiently run on an M2 Pro MacBook Pro with 16 GB memory.</p><h2 id="1-dependencies">1 Dependencies</h2><h3 id="11-ollama-and-llama-3-model">1.1 Ollama and Llama 3 Model</h3><p>Firstly, Ollama should be installed on a MacBook. Ollama can utilize the GPUs of the machine, ensuring efficient inference, provided there is sufficient memory. Llama 3-8B performs well on machines with 16 GB of memory.</p><div class="kg-card kg-callout-card kg-callout-card-grey"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Ollama can be downloaded here: <a href="https://ollama.com/?ref=localhost" target="_blank">https://ollama.com/</a></div></div><p>Once it is downloaded, can use below command in the terminal to pull the Llama 3-8B model:</p><pre><code class="language-shell">ollama pull llama3</code></pre><h3 id="12-python-dependencies">1.2 Python Dependencies</h3><p>Now, let's import the required packages to construct a RAG system with chat history, utilizing the LangChain toolkit.</p><pre><code class="language-python"># Models
from langchain.llms import LlamaCpp
from langchain.chat_models import ChatOpenAI

# Setup
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Vector store
from langchain.document_loaders import  TextLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangChain supports many other chat models. Here, we're using Ollama
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate

# RAG with Memory 
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory

# Display results
import markdown
from IPython.display import display, Markdown, Latex</code></pre><h2 id="2-create-vector-store">2 Create Vector Store</h2><p>The source data consists of&nbsp;<a href="https://vincentyuan.us/r/18a4e24d?m=df98d831-fc77-4019-8ad0-29db6fe20ba1&ref=localhost">a summary of important events and statistics</a> from the week of May 13th, 2024, as published by Yahoo Finance. This data is not included in the training set of Llama 3. For demonstration purposes, the news is extracted to a text file and utilized in the code to create the Chroma vector store and retriever.</p><pre><code class="language-python">source_data_path = '../data/yahoo.txt'

# for token-wise streaming so you'll see the answer gets generated token by token when Llama is answering your question
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

loader = TextLoader(source_data_path)

documents = loader.load()

#splitting the text into
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)

embedding = HuggingFaceEmbeddings()

vectordb = Chroma.from_documents(documents=texts,
                                 embedding=embedding
                                 # persist_directory=persist_directory
                                )
                                
retriever = vectordb.as_retriever(search_kwargs={"k": 5})</code></pre><h2 id="3-create-the-llm-object">3 Create the LLM Object</h2><p>Make sure the Ollama is on and the LLama 3 model has been downloaded, then below code can be used to define a LLM object in the pipeline:</p><pre><code class="language-python">llm = ChatOllama(model="llama3",
                temperature=0.1)</code></pre><h2 id="4-rag-with-memory">4 RAG with Memory</h2><p>In essence, there should be place to store chat history, also the the chat history is added to the prompt &nbsp;in RAG, so that the LLM can access past conversation, also the chat history is update after each round of conversation. Below is a way to use the&nbsp;<code>BaseChatMessageHistory</code>&nbsp;to address this need:</p><pre><code class="language-python">### Contextualize question ###
contextualize_q_system_prompt = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history. Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is."""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)


### Answer question ###
qa_system_prompt = """You are an assistant for question-answering tasks. \
Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. \
Use three sentences maximum and keep the answer concise.\

{context}"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)</code></pre><p>Then define the RAG chain:</p><pre><code class="language-python">### Statefully manage chat history ###
store = {}

def get_session_history(session_id: str) -&gt; BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)</code></pre><p>Then let's try if the model understands the Yahoo Finance analysis, the question is&nbsp;<code>What is the wall street expectation of the April Consumer Price Index (CPI)?</code>.</p><pre><code class="language-python">llm_response = conversational_rag_chain.invoke(
    {"input": "What is the wall street expectation of the April Consumer Price Index (CPI)?"},
    config={
        "configurable": {"session_id": "abc123"}
    },  # constructs a key "abc123" in `store`.
)["answer"]

print('='*50)
display(Markdown(llm_response))</code></pre><p>The response is:</p><pre><code class="language-shell">According to the text, Wall Street expects an annual gain of 3.4% for headline CPI, which includes the price of food and energy, a decrease from the 3.5% headline number in March. Additionally, prices are expected to rise 0.4% on a month-over-month basis, in line with March's rise.</code></pre><p>This is aligned with the source:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.jsdelivr.net/gh/BulletTech2021/Pics/img/1_V/CPI.png" class="kg-image" alt="" loading="lazy" width="1358" height="708"><figcaption><span style="white-space: pre-wrap;">Yahoo Finance Analysis</span></figcaption></figure><p>Then, a question is asked based on the output of last question to calculate the double of the expected CPI:</p><pre><code class="language-python">llm_response = conversational_rag_chain.invoke(
    {"input": "What is the double of the expected CPI in the prior answer?"},
    config={
        "configurable": {"session_id": "abc123"}
    },  # constructs a key "abc123" in `store`.
)["answer"]

print('='*50)
display(Markdown(llm_response))</code></pre><p>And this is the output:</p><pre><code class="language-shell">The expected annual gain for headline CPI is 3.4%. The double of this value would be:

2 x 3.4% = 6.8%

So, the double of the expected CPI is 6.8%.</code></pre><p>So the model successfully picks up the information that it returns in the past and answer correctly to the new question.</p><h2 id="5-summary">5 Summary</h2><p>This enhanced solution extends the capabilities of a regular RAG by supporting chat history, making it highly beneficial for multiple rounds of conversations. With Ollama, experiments like this can be run on an affordable laptop with embedded GPUs. A special acknowledgment to Meta for their great work in improving Llama 3.</p><p></p>
        </div>

        <div class="pagination-container gh-canvas">
        <nav class="pagination">

            <div class="pagination-left">
                    <a class="newer-posts" href="../build-a-regulation-assistant-powered-by-llama-2-and-streamlit-with-google-colab-gpus/index.html">
                        <span class="pagination-label">Previous</span>
                        Build a Regulation Assistant Powered by Llama 2 and Streamlit with Google Colab GPUs
                    </a>
            </div>

            <div class="pagination-right">
                    <a class="older-posts" href="../the-manic-scheme-in-payment-networksrehensive-analysis-of-transaction-ecosystems/index.html">
                        <span class="pagination-label">Next</span>
                        The MANIC Scheme in Payment Networks: A Comprehensive Analysis of Transaction Ecosystems
                    </a>
            </div>

        </nav>
        </div>


    </article>


</main>
    </div>

    <footer class="gh-foot gh-outer">
        <div class="gh-foot-inner gh-inner">
            <div class="gh-copyright">
                Spacecraft © 2025
            </div>
                <nav class="gh-foot-menu">
                    <ul class="nav">
    <li class="nav-sign-up"><a href="index.html#/portal/">Sign up</a></li>
</ul>

                </nav>
            <div class="gh-powered-by">
                <a href="https://ghost.org/" target="_blank" rel="noopener">Powered by Ghost</a>
            </div>
        </div>
    </footer>

</div>

    <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <div class="pswp__bg"></div>

    <div class="pswp__scroll-wrap">
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
<script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous">
</script>
<script src="../assets/built/main.min.js%3Fv=39200ba622"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.12.3/tocbot.min.js"></script>

<script>
    tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.gh-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.gh-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h2, h3, h4',
        // Ensure correct positioning
        hasInnerContainers: true,

        orderedList: false,
    });
</script>


<!-- Code highlights -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>


<!--   Scroll Top Button  -->
<script>
const scrollTopBtn = document.querySelector('.js-scroll-top');
if (scrollTopBtn) {
  scrollTopBtn.onclick = () => {
    window.scrollTo({top: 0, behavior: 'smooth'});
  }
  
  const progressPath = document.querySelector('.scroll-top path');
  const pathLength = progressPath.getTotalLength();
  progressPath.style.transition = progressPath.style.WebkitTransition = 'none';
  progressPath.style.strokeDasharray = `${pathLength} ${pathLength}`;
  progressPath.style.strokeDashoffset = pathLength;
  progressPath.getBoundingClientRect();
  progressPath.style.transition = progressPath.style.WebkitTransition = 'stroke-dashoffset 10ms linear';		
  const updateProgress = function() {
    const scroll = window.scrollY || window.scrollTopBtn || document.documentElement.scrollTopBtn;

    const docHeight = Math.max(
      document.body.scrollHeight, document.documentElement.scrollHeight,
      document.body.offsetHeight, document.documentElement.offsetHeight,
      document.body.clientHeight, document.documentElement.clientHeight
    );

    const windowHeight = Math.max(document.documentElement.clientHeight, window.innerHeight || 0);

    const height = docHeight - windowHeight;
    var progress = pathLength - (scroll * pathLength / height);
    progressPath.style.strokeDashoffset = progress;
  }

  updateProgress();
  const offset = 100;

  window.addEventListener('scroll', function(event) {
    updateProgress();

    //Scroll back to top
    const scrollPos = window.scrollY || window.scrollTopBtn || document.getElementsByTagName('html')[0].scrollTopBtn;
    scrollPos > offset ? scrollTopBtn.classList.add('is-active') : scrollTopBtn.classList.remove('is-active');

  }, false);
}
</script>

</body>
</html>