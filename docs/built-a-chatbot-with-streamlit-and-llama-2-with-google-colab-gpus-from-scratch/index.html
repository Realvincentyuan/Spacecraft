<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Built a Chatbot with Streamlit and Llama 2 with Google Colab GPUs from Scratch</title>
    <link rel="stylesheet" href="../assets/built/screen.css%3Fv=88df665c6b.css">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.12.3/tocbot.css">

    <style>
        .gh-content {
            position: relative;
            z-index: 1;
        }

        /*-- Remove underline in the TOC text  */
        .gh-content a {
            text-decoration: none;
        }

        .gh-content img{
            position: relative;
            z-index: 2;
        }

        .gh-toc > .toc-list {
            position: relative;
            
        }

        .toc-list {
            overflow: hidden;
            list-style: none;
            left: 20%;

        }

        @media (min-width: 1300px) {
            .gh-sidebar {
                position: absolute; 
                top: 0;
                bottom: 0;
                margin-top: 4vmin;
                /* grid-column: wide-start / main-start;  Place the TOC to the left of the content */
                grid-column: main-end / wide-end ; /* Place the TOC to the right of the content */

            }
        
            .gh-toc {
                position: sticky; /* On larger screens, TOC will stay in the same spot on the page */
                top: 4vmin;

                /*-- Send the TOC to the back, in case it is overlapped with images  */
                z-index: 1; 

            }
        }

        /*-- Hide TOC on mobile devices */
        @media (max-width: 768px) {
            .gh-toc {
                display: none;
            }
        }

        .gh-toc .is-active-link::before {
            background-color: var(--ghost-accent-color); /* Defines TOC   accent color based on Accent color set in Ghost Admin */

        } 
    </style>

    <meta name="description" content="End to end chatbot development with Llama 2.">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Spacecraft">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Built a Chatbot with Streamlit and Llama 2 with Google Colab GPUs from Scratch">
    <meta property="og:description" content="End to end chatbot development with Llama 2.">
    <meta property="og:url" content="https://realvincentyuan.github.io/Spacecraft/built-a-chatbot-with-streamlit-and-llama-2-with-google-colab-gpus-from-scratch/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta property="article:published_time" content="2024-07-07T20:31:52.000Z">
    <meta property="article:modified_time" content="2024-07-07T20:33:05.000Z">
    <meta property="article:tag" content="GenAI">
    
    <meta property="article:publisher" content="https://www.facebook.com/ghost">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Built a Chatbot with Streamlit and Llama 2 with Google Colab GPUs from Scratch">
    <meta name="twitter:description" content="End to end chatbot development with Llama 2.">
    <meta name="twitter:url" content="https://realvincentyuan.github.io/Spacecraft/built-a-chatbot-with-streamlit-and-llama-2-with-google-colab-gpus-from-scratch/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Vincent Yuan">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="GenAI">
    <meta name="twitter:site" content="@ghost">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="823">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Spacecraft",
        "url": "https://realvincentyuan.github.io/Spacecraft/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://realvincentyuan.github.io/Spacecraft/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Vincent Yuan",
        "image": {
            "@type": "ImageObject",
            "url": "https://realvincentyuan.github.io/Spacecraft/content/images/2024/07/avatar.png",
            "width": 752,
            "height": 752
        },
        "url": "https://realvincentyuan.github.io/Spacecraft/author/vincent/",
        "sameAs": []
    },
    "headline": "Built a Chatbot with Streamlit and Llama 2 with Google Colab GPUs from Scratch",
    "url": "https://realvincentyuan.github.io/Spacecraft/built-a-chatbot-with-streamlit-and-llama-2-with-google-colab-gpus-from-scratch/",
    "datePublished": "2024-07-07T20:31:52.000Z",
    "dateModified": "2024-07-07T20:33:05.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&ixlib=rb-4.0.3&q=80&w=2000",
        "width": 1200,
        "height": 823
    },
    "keywords": "GenAI",
    "description": "End to end chatbot development with Llama 2.",
    "mainEntityOfPage": "https://realvincentyuan.github.io/Spacecraft/built-a-chatbot-with-streamlit-and-llama-2-with-google-colab-gpus-from-scratch/"
}
    </script>

    <meta name="generator" content="Ghost 5.87">
    <link rel="alternate" type="application/rss+xml" title="Spacecraft" href="../rss/index.html">
    
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/sodo-search.min.js" data-key="59658b4dc0364cacbc3332171d" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/main.css" data-sodo-search="https://realvincentyuan.github.io/Spacecraft/" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/ghost/announcement-bar@~1.1/umd/announcement-bar.min.js" data-announcement-bar="https://realvincentyuan.github.io/Spacecraft/" data-api-url="https://realvincentyuan.github.io/Spacecraft/members/api/announcement/" crossorigin="anonymous"></script>
    <link href="https://realvincentyuan.github.io/Spacecraft/webmentions/receive/" rel="webmention">
    <script defer src="../public/cards.min.js%3Fv=88df665c6b"></script>
    <link rel="stylesheet" type="text/css" href="../public/cards.min.css%3Fv=88df665c6b.css">
    <script defer src="../public/comment-counts.min.js%3Fv=88df665c6b" data-ghost-comments-counts-api="https://realvincentyuan.github.io/Spacecraft/members/api/comments/counts/"></script><style>:root {--ghost-accent-color: #0f69c2;}</style>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-633EG809GP"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-633EG809GP');
</script>

<!-- Reading progress bar -->
<style>
.reading-progress {
  position: fixed;
  top: 0;
  z-index: 999;
  width: 100%;
  height: 5px; /* Progress bar height */
  background: #c5d2d9; /* Progress bar background color */
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; /* Hide default progress bar */
}

.reading-progress::-webkit-progress-bar {
  background-color: transparent;
}

.reading-progress::-webkit-progress-value {
  background: var(--ghost-accent-color); /* Progress bar color */
}
</style>


<!--   Scroll Top Button  -->
<style>
    /*Scroll to Top CSS*/
    .scroll-top {
		position: fixed;
 		z-index: 50;
		padding: 0;
		right: 30px;
		bottom: 100px;
		opacity: 0;
		visibility: hidden;
		transform: translateY(15px);    
		height: 46px;
		width: 46px;
		cursor: pointer;
		display: flex;
  		align-items: center;
  		justify-content: center;
		border-radius: 50%;
  	transition: all .4s ease;
  	border: none;
  	box-shadow: inset 0 0 0 2px #ccc;
  	color: #ccc;
  	background-color: #fff;
	}

	.scroll-top.is-active {
		opacity: 1;
  		visibility: visible;
  		transform: translateY(0);
	}

	.scroll-top .icon-tabler-arrow-up {
  		position: absolute;
  		stroke-width: 2px;
  		stroke: #333;
	}

	.scroll-top svg path { 
		fill: none; 
	}

	.scroll-top svg.progress-circle path {
		stroke: #333;
		stroke-width: 4;
  		transition: all .4s ease;
	}

	.scroll-top:hover {
  		color: var(--ghost-accent-color);
	}

	.scroll-top:hover .progress-circle path, .scroll-top:hover .icon-tabler-arrow-up {
  		stroke: var(--ghost-accent-color);
	}
          
</style>

    <button class="btn-toggle-round scroll-top js-scroll-top" type="button" title="Scroll to top">
      <svg class="progress-circle" width="100%" height="100%" viewBox="-1 -1 102 102">
        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
      </svg>
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-arrow-up" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="cuurentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <line x1="12" y1="5" x2="12" y2="19" />
        <line x1="18" y1="11" x2="12" y2="5" />
        <line x1="6" y1="11" x2="12" y2="5" />
      </svg>
    </button>



<!-- Code highlights -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>

<body class="post-template tag-genai  is-head-left-logo">
<div class="site">

    <header id="gh-head" class="gh-head gh-outer">
        <div class="gh-head-inner gh-inner">
            <div class="gh-head-brand">
                <div class="gh-head-brand-wrapper">
                    <a class="gh-head-logo" href="../index.html">
                            Spacecraft
                    </a>
                </div>
                <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M17.5 17.5L12.5 12.5L17.5 17.5ZM14.1667 8.33333C14.1667 9.09938 14.0158 9.85792 13.7226 10.5657C13.4295 11.2734 12.9998 11.9164 12.4581 12.4581C11.9164 12.9998 11.2734 13.4295 10.5657 13.7226C9.85792 14.0158 9.09938 14.1667 8.33333 14.1667C7.56729 14.1667 6.80875 14.0158 6.10101 13.7226C5.39328 13.4295 4.75022 12.9998 4.20854 12.4581C3.66687 11.9164 3.23719 11.2734 2.94404 10.5657C2.65088 9.85792 2.5 9.09938 2.5 8.33333C2.5 6.78624 3.11458 5.30251 4.20854 4.20854C5.30251 3.11458 6.78624 2.5 8.33333 2.5C9.88043 2.5 11.3642 3.11458 12.4581 4.20854C13.5521 5.30251 14.1667 6.78624 14.1667 8.33333Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</button>
                <button class="gh-burger"></button>
            </div>

            <nav class="gh-head-menu">
                <ul class="nav">
    <li class="nav-home"><a href="../index.html">Home</a></li>
    <li class="nav-tech"><a href="../tag/tech/index.html">Tech</a></li>
    <li class="nav-profession"><a href="../tag/pro/index.html">Profession</a></li>
    <li class="nav-life"><a href="../tag/life/index.html">Life</a></li>
    <li class="nav-about"><a href="../about/index.html">About</a></li>
</ul>

            </nav>

            <div class="gh-head-actions">
                        <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M17.5 17.5L12.5 12.5L17.5 17.5ZM14.1667 8.33333C14.1667 9.09938 14.0158 9.85792 13.7226 10.5657C13.4295 11.2734 12.9998 11.9164 12.4581 12.4581C11.9164 12.9998 11.2734 13.4295 10.5657 13.7226C9.85792 14.0158 9.09938 14.1667 8.33333 14.1667C7.56729 14.1667 6.80875 14.0158 6.10101 13.7226C5.39328 13.4295 4.75022 12.9998 4.20854 12.4581C3.66687 11.9164 3.23719 11.2734 2.94404 10.5657C2.65088 9.85792 2.5 9.09938 2.5 8.33333C2.5 6.78624 3.11458 5.30251 4.20854 4.20854C5.30251 3.11458 6.78624 2.5 8.33333 2.5C9.88043 2.5 11.3642 3.11458 12.4581 4.20854C13.5521 5.30251 14.1667 6.78624 14.1667 8.33333Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</button>
            </div>
        </div>
    </header>


    <div class="site-content">
        
<main class="site-main">


    <article class="post tag-genai featured">

        <header class="gh-article-header gh-canvas">

        <div class="article-tag post-card-tags">
                <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
        </div>

            <h1 class="gh-article-title">Built a Chatbot with Streamlit and Llama 2 with Google Colab GPUs from Scratch</h1>

            <div class="post-meta">
                <a href="../author/vincent/index.html">Vincent Yuan</a>&nbsp;Â·&nbsp;updated on&nbsp;
                <time datetime="2024-07-07">Jul 7, 2024</time>&nbsp;Â·&nbsp;<span class="gh-card-length">5 min read</span>
            </div>

            <div class="post-meta">
                    <a class="post-tag tag-genai" href="../tag/genai/index.html" title="GenAI">GenAI</a>
            </div>

                <figure class="gh-article-image">
        <img
            class="post-image"
            srcset="https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;400 400w,
                    https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;750 750w,
                    https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;960 960w,
                    https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1140 1140w,
                    https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1920 1920w"
            sizes="(min-width: 1200px) 960px, 92vw"
            src="https://images.unsplash.com/photo-1715394016216-f0c45a43ca69?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQ5fHx2eTEyMTh8ZW58MHx8fHwxNzIwMzgzNjQ1fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;960"
            alt="Built a Chatbot with Streamlit and Llama 2 with Google Colab GPUs from Scratch"
        >
            <figcaption><span style="white-space: pre-wrap;">Photo by </span><a href="https://unsplash.com/@vincentyuan87?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span style="white-space: pre-wrap;">Vincent Yuan @USA</span></a><span style="white-space: pre-wrap;"> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span style="white-space: pre-wrap;">Unsplash</span></a></figcaption>
    </figure>
        </header>

        <div class="gh-content gh-canvas">

            <aside class="gh-sidebar"><div class="gh-toc"></div></aside> 

            <p>So far, we have talked about a lot of things regarding Llama 2:</p><ul><li>Swift inference powered by GPUs</li><li>Thoughtful responses with appropriate prompts</li><li>Question answering utilizing a knowledge database</li><li>A user-friendly web interface</li></ul><p>You can find those informative posts in the&nbsp;<code>GenAI</code>&nbsp;section of Spacecraft as below:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://realvincentyuan.github.io/Spacecraft/tag/genai/index.html?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GenAI - Spacecraft</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://t1.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;fallback_opts=TYPE,SIZE,URL&amp;url=https://github.io/Spacecraft/tag/genai/index.html&amp;size=128" alt=""><span class="kg-bookmark-author">Spacecraft</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://images.unsplash.com/photo-1477959858617-67f85cf4f1df?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fGNoaWNhZ298ZW58MHx8fHwxNjg4OTQxMDIwfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt=""></div></a></figure><p>This post shows a product that makes the best of all the things learned, and build a web-based chatbot powered by a local Llama 2 model, running on Google Colab with GPUs.</p><h2 id="2-dependencies">2 Dependencies</h2><p>Firstly, install a few dependencies:</p><pre><code class="language-py">!pip install -q streamlit

!npm install localtunnel

# GPU setup of LangChain
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --force-reinstall llama-cpp-python==0.2.28  --no-cache-dir

!pip install huggingface_hub  chromadb langchain sentence-transformers pinecone_client</code></pre><p>Then download the Llama 2 model to the Colab notebook:</p><pre><code class="language-py">!wget https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf</code></pre><h2 id="3-build-the-web-chatbot">3 Build the Web Chatbot</h2><p>The web chatbot is like this:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.jsdelivr.net/gh/BulletTech2021/Pics/img/1_V/llama_2_bot.png" class="kg-image" alt="" loading="lazy" width="2998" height="1918"><figcaption><span style="white-space: pre-wrap;">Llama 2 Chatbot</span></figcaption></figure><p>You need to write the app code to the disk first:</p><pre><code class="language-py">%%writefile app.py

import streamlit as st
import os

from langchain.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.prompts import PromptTemplate

from langchain.llms import LlamaCpp


# App title
st.set_page_config(page_title="ðŸ¦™ðŸ’¬ Llama 2 Chatbot")

llama_model_path = 'llama-2-7b-chat.Q5_0.gguf'

n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.
n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.

# for token-wise streaming so you'll see the answer gets generated token by token when Llama is answering your question
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

# Replicate Credentials
with st.sidebar:
    st.title('ðŸ¦™ðŸ’¬ Llama 2 Chatbot')


    st.subheader('Models and parameters')
    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')

    if selected_model == 'Llama2-7B':
        llm_path = llama_model_path
    elif selected_model == 'Llama2-13B':
        llm_path = llama_model_path

    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)
    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)
    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)
    st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')


    llm = LlamaCpp(
      model_path=llm_path,
      temperature=temperature,
      top_p=top_p,
      n_ctx=2048,
      n_gpu_layers=n_gpu_layers,
      n_batch=n_batch,
      callback_manager=callback_manager,
      verbose=True,
    )

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)


# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot
def generate_llama2_response(prompt_input):

    pre_prompt = """[INST] &lt;&lt;SYS&gt;&gt;
                  You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.

                  If you cannot answer the question from the given documents, please state that you do not have an answer.\n
                  """


    for dict_message in st.session_state.messages:
        if dict_message["role"] == "user":
            pre_prompt += "User: " + dict_message["content"] + "\n\n"
        else:
            pre_prompt += "Assistant: " + dict_message["content"] + "\n\n"

    prompt = pre_prompt +  "User : {question}" + "[\INST]"
    llama_prompt = PromptTemplate(template=prompt, input_variables=["question"])

    chain = LLMChain(llm=llm, prompt=llama_prompt)

    result = chain({
                "question": prompt_input
                 })


    return result['text']

# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama2_response(prompt)
            placeholder = st.empty()
            full_response = ''
            for item in response:
                full_response += item
                placeholder.markdown(full_response)
            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)</code></pre><h3 id="31-model-setup">3.1 Model Setup</h3><p>In the code, firstly tweak the params per your hardware, models and objectives:</p><pre><code class="language-py">llama_model_path = 'llama-2-7b-chat.Q5_0.gguf'

n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.
n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.</code></pre><p>The free version of Colab does not too much memory so here the&nbsp;<code>llama-2-7b-chat.Q5_0.gguf</code>&nbsp;is used but you can use a larger model for better performance.</p><h3 id="32-message-management">3.2 Message Management</h3><p>Then, these are the setup for the display/clear of messages of the chatbot:</p><pre><code class="language-py"># Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)</code></pre><h3 id="33-get-llm-response">3.3 Get LLM Response</h3><p>Below function appends the chat history into the prompt and get the response of model:</p><pre><code class="language-py">def generate_llama2_response(prompt_input):

    pre_prompt = """[INST] &lt;&lt;SYS&gt;&gt;
                  You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.

                  If you cannot answer the question from the given documents, please state that you do not have an answer.\n
                  """


    for dict_message in st.session_state.messages:
        if dict_message["role"] == "user":
            pre_prompt += "User: " + dict_message["content"] + "\n\n"
        else:
            pre_prompt += "Assistant: " + dict_message["content"] + "\n\n"

    prompt = pre_prompt +  "User : {question}" + "[\INST]"
    llama_prompt = PromptTemplate(template=prompt, input_variables=["question"])

    chain = LLMChain(llm=llm, prompt=llama_prompt)

    result = chain({
                "question": prompt_input
                 })


    return result['text']</code></pre><h3 id="34-conversation">3.4 Conversation</h3><p>Below shows the question and answering process, the chatbot responses to users' questions:</p><pre><code class="language-py"># User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama2_response(prompt)
            placeholder = st.empty()
            full_response = ''
            for item in response:
                full_response += item
                placeholder.markdown(full_response)
            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)</code></pre><h2 id="4-start-the-chatbot">4 Start the Chatbot</h2><p>You can bring up the chatbot by using below command:</p><pre><code class="language-py">!streamlit run app.py --server.address=localhost &amp;&gt;/content/logs.txt &amp;

import urllib
print("Password/Enpoint IP for localtunnel is:",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))

!npx localtunnel --port 8501</code></pre><p>The result shows a password to access the web app:</p><pre><code class="language-py">Password/Enpoint IP for localtunnel is: 35.185.197.1
npx: installed 22 in 2.393s
your url is: https://hot-pets-chew.loca.lt</code></pre><p>Go to that url and enter the password, and enjoy the time!</p><h2 id="5-conclusion">5 Conclusion</h2><p>This post consolidates information to transform your local Llama 2 model into a fully functional chatbot. Moreover, you have the flexibility to craft specialized assistants for distinct domains by customizing the system prompts, all at no additional cost.</p><p>Let's build something cool!</p>
        </div>

        <div class="pagination-container gh-canvas">
        <nav class="pagination">

            <div class="pagination-left">
                    <a class="newer-posts" href="../question-answering-on-multiple-files-with-llama-2-and-rag/index.html">
                        <span class="pagination-label">Previous</span>
                        Question Answering on Multiple Files with Llama 2 and RAG
                    </a>
            </div>

            <div class="pagination-right">
                    <a class="older-posts" href="../unveiling-the-deal-what-happens-when-companies-merge/index.html">
                        <span class="pagination-label">Next</span>
                        Unveiling the Deal: What Happens When Companies Merge
                    </a>
            </div>

        </nav>
        </div>

            <div class="gh-comments gh-canvas">
                
        <script defer src="https://cdn.jsdelivr.net/ghost/comments-ui@~0.17/umd/comments-ui.min.js" data-ghost-comments="https://realvincentyuan.github.io/Spacecraft/" data-api="https://realvincentyuan.github.io/Spacecraft/ghost/api/content/" data-admin="https://realvincentyuan.github.io/Spacecraft/ghost/" data-key="59658b4dc0364cacbc3332171d" data-title="null" data-count="true" data-post-id="668afae6ac15d470add4a506" data-color-scheme="auto" data-avatar-saturation="60" data-accent-color="#0f69c2" data-comments-enabled="all" data-publication="Spacecraft" crossorigin="anonymous"></script>
    
            </div>

    </article>


</main>
    </div>

    <footer class="gh-foot gh-outer">
        <div class="gh-foot-inner gh-inner">
            <div class="gh-copyright">
                Spacecraft Â© 2024
            </div>
                <nav class="gh-foot-menu">
                    <ul class="nav">
    <li class="nav-sign-up"><a href="index.html#/portal/">Sign up</a></li>
</ul>

                </nav>
            <div class="gh-powered-by">
                <a href="https://ghost.org/" target="_blank" rel="noopener">Powered by Ghost</a>
            </div>
        </div>
    </footer>

</div>

    <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <div class="pswp__bg"></div>

    <div class="pswp__scroll-wrap">
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
<script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous">
</script>
<script src="../assets/built/main.min.js%3Fv=88df665c6b"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.12.3/tocbot.min.js"></script>

<script>
    tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.gh-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.gh-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h2, h3, h4',
        // Ensure correct positioning
        hasInnerContainers: true,

        orderedList: false,
    });
</script>


<!-- Code highlights -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-fTl/qcO1VgvKtOMApX2PdZzkziyr2stM65GYPLGuYMnuMm1z2JLJG6XVU7C/mR+E7xBUqCivykuhlzfqxXBXbg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>


<!--   Scroll Top Button  -->
<script>
const scrollTopBtn = document.querySelector('.js-scroll-top');
if (scrollTopBtn) {
  scrollTopBtn.onclick = () => {
    window.scrollTo({top: 0, behavior: 'smooth'});
  }
  
  const progressPath = document.querySelector('.scroll-top path');
  const pathLength = progressPath.getTotalLength();
  progressPath.style.transition = progressPath.style.WebkitTransition = 'none';
  progressPath.style.strokeDasharray = `${pathLength} ${pathLength}`;
  progressPath.style.strokeDashoffset = pathLength;
  progressPath.getBoundingClientRect();
  progressPath.style.transition = progressPath.style.WebkitTransition = 'stroke-dashoffset 10ms linear';		
  const updateProgress = function() {
    const scroll = window.scrollY || window.scrollTopBtn || document.documentElement.scrollTopBtn;

    const docHeight = Math.max(
      document.body.scrollHeight, document.documentElement.scrollHeight,
      document.body.offsetHeight, document.documentElement.offsetHeight,
      document.body.clientHeight, document.documentElement.clientHeight
    );

    const windowHeight = Math.max(document.documentElement.clientHeight, window.innerHeight || 0);

    const height = docHeight - windowHeight;
    var progress = pathLength - (scroll * pathLength / height);
    progressPath.style.strokeDashoffset = progress;
  }

  updateProgress();
  const offset = 100;

  window.addEventListener('scroll', function(event) {
    updateProgress();

    //Scroll back to top
    const scrollPos = window.scrollY || window.scrollTopBtn || document.getElementsByTagName('html')[0].scrollTopBtn;
    scrollPos > offset ? scrollTopBtn.classList.add('is-active') : scrollTopBtn.classList.remove('is-active');

  }, false);
}
</script>

</body>
</html>